---
title: Introduction to Transformer Models
description: Learn about the architecture that powers modern NLP models like GPT and BERT.
date: 2024-01-15
category: technical
published: true
---

# Introduction to Transformer Models

Transformer models have revolutionized natural language processing (NLP) since their introduction in the 2017 paper "Attention Is All You Need" by Vaswani et al. They've become the foundation for powerful language models like BERT, GPT, and T5.

## Why Transformers Matter

Before transformers, recurrent neural networks (RNNs) like LSTMs were the dominant architecture for sequence tasks. However, RNNs had limitations:

1. **Sequential processing** - RNNs process text word by word, making them slow to train
2. **Limited context window** - Difficulty capturing long-range dependencies
3. **Vanishing gradient problem** - Information from early words can be lost in long sequences

Transformers addressed these issues through a novel architecture based entirely on attention mechanisms.

## Core Components of the Transformer Architecture

### 1. Self-Attention Mechanism

The key innovation in transformers is the **self-attention mechanism**, which allows the model to focus on different parts of the input sequence when encoding each word. This enables:

- Parallel processing (vs. sequential in RNNs)
- Better handling of long-range dependencies
- More interpretable models

The self-attention calculation involves:

1. Creating query (Q), key (K), and value (V) matrices from the input
2. Computing attention scores: `Attention(Q, K, V) = softmax(QK^T/âˆšd_k)V`

### 2. Multi-Head Attention

Rather than performing a single attention function, transformers use **multi-head attention**, which:

- Allows the model to attend to information from different representation subspaces
- Provides multiple "perspectives" on the sequence

### 3. Positional Encoding

Since transformers process all words in parallel, they need a way to understand word order. **Positional encodings** are added to the input embeddings to provide this information, using sine and cosine functions of different frequencies.

## Encoder-Decoder Architecture

The original transformer uses an encoder-decoder structure:

- **Encoder**: Processes the input sequence and creates representations
- **Decoder**: Generates the output sequence based on the encoder representations

Each encoder/decoder block consists of:
1. Multi-head attention layer
2. Feed-forward neural network
3. Layer normalization and residual connections

## Popular Transformer-Based Models

### BERT (Bidirectional Encoder Representations from Transformers)

- Uses only the encoder portion of the transformer
- Pre-trained on masked language modeling and next sentence prediction
- Bidirectional context for each word
- Great for understanding tasks like classification, named entity recognition

### GPT (Generative Pre-trained Transformer)

- Uses primarily the decoder portion of the transformer
- Auto-regressive: predicts the next token based on previous tokens
- Unidirectional attention (each word can only attend to previous words)
- Excellent for text generation tasks

## Implementing a Simple Transformer in PyTorch

Here's a simplified implementation of a transformer encoder layer:

```python
import torch
import torch.nn as nn

class TransformerEncoderLayer(nn.Module):
    def __init__(self, d_model, nhead, dim_feedforward, dropout=0.1):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(dim_feedforward, d_model)
        
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        
        self.activation = nn.ReLU()
    
    def forward(self, src, src_mask=None, src_key_padding_mask=None):
        # Self-attention block
        src2 = self.self_attn(src, src, src, attn_mask=src_mask,
                              key_padding_mask=src_key_padding_mask)[0]
        src = src + self.dropout1(src2)
        src = self.norm1(src)
        
        # Feed-forward block
        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
        src = src + self.dropout2(src2)
        src = self.norm2(src)
        return src
```

## Conclusion

Transformers have become the backbone of modern NLP and are increasingly used in computer vision, reinforcement learning, and even biological sequence analysis. Their ability to process sequences in parallel and capture long-range dependencies has made them incredibly powerful.

As an ML engineer, understanding transformer architecture is essential for working with state-of-the-art models and building advanced applications in natural language processing and beyond.

## Further Reading

1. "Attention Is All You Need" (Vaswani et al., 2017)
2. "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" (Devlin et al., 2018)
3. "The Illustrated Transformer" by Jay Alammar (blog post)
4. "The Annotated Transformer" (Harvard NLP)